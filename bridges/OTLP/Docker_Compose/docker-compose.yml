# =============================================================================
# PLTelemetry Master Stack - Everything in One Place
# =============================================================================
#
# Complete PLTelemetry ecosystem with observability, infrastructure and examples
# 
# Stack includes:
# - Observability: Grafana + Tempo + Loki + Prometheus + OTEL Collector
# - Infrastructure: Oracle DB + MailHog  
# - Examples: All Node.js microservices + Financial API
#
# Usage:
#   docker compose up -d
#   Access:
#     - Grafana: http://localhost:3020 (admin/admin)
#     - Oracle DB: localhost:1521 (system/plt)
#     - MailHog: http://localhost:8025
#     - Example Services: 8001-8005
#
# =============================================================================

services:
  # =============================================================================
  # INFRASTRUCTURE - Oracle Database (Community Image)
  # =============================================================================
  
  oracle-db:
    image: gvenzl/oracle-free:latest
    container_name: oracle-plt
    hostname: oracle-db
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 16G
        reservations:
          cpus: '4.0'
          memory: 12G
    environment:
      - ORACLE_PASSWORD=plt
      - APP_USER=plt
      - APP_USER_PASSWORD=plt
      - ORACLE_DATABASE=FREEPDB1
    ports:
      - "1521:1521"     # Database listener
    volumes:
      # Using local volumes instead of external ones
      - oracle-data:/opt/oracle/oradata
      - oracle-backup:/opt/oracle/backup
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "sqlplus", "-L", "plt/plt@//localhost:1521/FREEPDB1", "<<<", "SELECT 1 FROM DUAL;"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  # =============================================================================
  # OBSERVABILITY STACK - Grafana + Tempo + Loki + Prometheus + OTEL
  # =============================================================================
  
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: plt-otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./configs/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver (PLTelemetry uses this)
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    depends_on:
      - tempo
      - loki
      - prometheus
    networks:
      - pltelemetry-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1200M
        reservations:
          cpus: '1.0'
          memory: 600M
    environment:
      - GOMEMLIMIT=1100MiB
      - GOGC=75

  tempo:
    image: grafana/tempo:latest
    container_name: plt-tempo
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./configs/tempo.yaml:/etc/tempo.yaml:ro
      - tempo-data:/var/tempo
    ports:
      - "3200:3200"   # Tempo API
      - "9411:9411"   # Zipkin receiver
    networks:
      - pltelemetry-network
    restart: unless-stopped
    user: "0"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 600M        # 15% de la RAM
        reservations:
          cpus: '0.3'
          memory: 300M

  loki:
    image: grafana/loki:latest
    container_name: plt-loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./configs/loki.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    ports:
      - "3100:3100"   # Loki API
    networks:
      - pltelemetry-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 600M        # 15% de la RAM
        reservations:
          cpus: '0.3'
          memory: 300M

  prometheus:
    image: prom/prometheus:latest
    container_name: plt-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'    # 30 días OK con 23GB
      - '--storage.tsdb.retention.size=8GB'    # Máximo 8GB (35% del disco)
      - '--query.max-concurrency=20'
      - '--query.max-samples=30000000'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1000M       # 25% de la RAM
        reservations:
          cpus: '0.5'
          memory: 500M
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"   # Prometheus UI
    networks:
      - pltelemetry-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: plt-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      # SMTP Configuration for email alerts
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=mailhog:1025
      - GF_SMTP_SKIP_VERIFY=true
      - GF_SMTP_FROM_ADDRESS=pltelemetry@grafana.local
      - GF_SMTP_FROM_NAME=PLTelemetry Alerts
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 400M        # 10% de la RAM
        reservations:
          cpus: '0.2'
          memory: 200M
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3020:3000"   # Grafana UI
    depends_on:
      - tempo
      - loki
      - prometheus
      - mailhog
    networks:
      - pltelemetry-network
    restart: unless-stopped

  # =============================================================================
  # COMMUNICATION - MailHog for testing emails
  # =============================================================================
  
  mailhog:
    image: mailhog/mailhog:latest
    container_name: plt-mailhog
    ports:
      - "1025:1025"   # SMTP port
      - "8025:8025"   # Web UI port
    networks:
      - pltelemetry-network
    restart: unless-stopped

  # =============================================================================
  # EXAMPLE SERVICES - From example_02 (working services)
  # =============================================================================
  
  oracle-reports:
    build: .  # Uses local Dockerfile and services/
    container_name: plt-oracle-reports
    ports:
      - "8001:8001"
    environment:
      - SERVICE_NAME=oracle-reports
      - SERVICE_PORT=8001
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    command: npm run start:reports
    restart: unless-stopped
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  weblogic-erp:
    build: .  # Uses local Dockerfile and services/
    container_name: plt-weblogic-erp
    ports:
      - "8002:8002"
    environment:
      - SERVICE_NAME=weblogic-erp
      - SERVICE_PORT=8002
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    command: npm run start:weblogic
    restart: unless-stopped
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  email-service:
    build: .  # Uses local Dockerfile and services/
    container_name: plt-email-service
    ports:
      - "8003:8003"
    environment:
      - SERVICE_NAME=email-service
      - SERVICE_PORT=8003
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    command: npm run start:email
    restart: unless-stopped
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  batch-processor:
    build: .  # Uses local Dockerfile and services/
    container_name: plt-batch-processor
    ports:
      - "8004:8004"
    environment:
      - SERVICE_NAME=batch-processor
      - SERVICE_PORT=8004
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    command: npm run start:batch
    restart: unless-stopped
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  document-service:
    build: .  # Uses local Dockerfile and services/
    container_name: plt-document-service
    ports:
      - "8005:8005"
    environment:
      - SERVICE_NAME=document-service
      - SERVICE_PORT=8005
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    command: npm run start:documents
    restart: unless-stopped
    networks:
      - pltelemetry-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3

# =============================================================================
# VOLUMES - All Local Now
# =============================================================================
volumes:
  # Oracle volumes (local instead of external)
  oracle-data:
    driver: local
  oracle-backup:
    driver: local
  
  # Observability volumes
  tempo-data:
    driver: local
  loki-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

# =============================================================================
# NETWORK - One Network to Rule Them All
# =============================================================================
networks:
  pltelemetry-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16
